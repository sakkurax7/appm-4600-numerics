{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/cu-applied-math/appm-4600-numerics/blob/main/Demos/Ch1_AutoDiff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GplvV8syNQtW"
   },
   "source": [
    "# Automatic Differentiation demo\n",
    "Using Jax and PyTorch\n",
    "\n",
    "APPM 4600\n",
    "\n",
    "Copyright Dept of Applied Math, University of Colorado Boulder. Released under a BSD 3-clause license\n",
    "\n",
    "Learning objectives:\n",
    "1. See how to use AutoDiff using two popular frameworks (jax and PyTorch)\n",
    "2. See that reverse mode is usually faster (than forward mode) for functions $f:\\mathbb{R}^n \\to \\mathbb{R}$\n",
    "3. Compare to symbolic differentiation\n",
    "\n",
    "Further reading\n",
    "- another [AutoDiff](https://github.com/cu-applied-math/SciML-Class/blob/main/Demos/AutomaticDifferentiation.ipynb) demo from CU\n",
    "- [JAX](https://docs.jax.dev/en/latest/index.html)\n",
    "- [PyTorch](https://pytorch.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIlDxOqfNZBO"
   },
   "source": [
    "## using jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eQiN5MqMMJDP"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import jacfwd, jacrev\n",
    "from jax import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yl89jUE8RB0P"
   },
   "source": [
    "We'll make a simple function. Note that the \"@\" sign is matrix multiplication (for either jax or numpy), i.e., [jax.numpy.matmul](https://docs.jax.dev/en/latest/_autosummary/jax.numpy.matmul.html#jax.numpy.matmul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1RanQp4iMSU-"
   },
   "outputs": [],
   "source": [
    "n = int(1e1)\n",
    "m = int(n/2)\n",
    "\n",
    "# We want some arbitrary matrix -- e.g., we could do this randomly\n",
    "# jax has some utilities for this, but if you don't want to learn them,\n",
    "# just convert from numpy\n",
    "A = np.random.randn(m,n)\n",
    "x = np.random.randn(n,1)\n",
    "A = jnp.array(A)\n",
    "x = jnp.array(x)\n",
    "\n",
    "def f(x):\n",
    "    return jnp.sum( A @ x )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_Tok6FoNhLm"
   },
   "source": [
    "We can ask jax for:\n",
    "- the gradient (of a function $f: \\mathbb{R}^n \\to \\mathbb{R}$)\n",
    "- the Jacobian (of a function $f: \\mathbb{R}^n \\to \\mathbb{R}^m$)\n",
    "  - if $m=1$ this *is* the gradient! (though sometimes there is a transpose difference...)\n",
    "\n",
    "Gradients are always computed via **reverse mode**, but for Jacobians, you can choose either **reverse** or **forward** mode. In general, if $n > m$ you want **reverse** mode. See Jax's [\"Autodiff Cookbook\"](https://docs.jax.dev/en/latest/notebooks/autodiff_cookbook.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5FqkGJkWNBsb",
    "outputId": "105f40cf-772d-41de-8793-d6c1f8d6c79c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[ 1.9309464 ],\n",
       "        [ 1.7242033 ],\n",
       "        [-0.42367828],\n",
       "        [-0.02427091],\n",
       "        [-2.9808059 ],\n",
       "        [ 1.0146751 ],\n",
       "        [-0.27772245],\n",
       "        [-3.8455067 ],\n",
       "        [-1.8126769 ],\n",
       "        [-2.2204695 ]], dtype=float32),\n",
       " Array([[ 1.9309464 ],\n",
       "        [ 1.7242033 ],\n",
       "        [-0.42367828],\n",
       "        [-0.02427091],\n",
       "        [-2.9808059 ],\n",
       "        [ 1.0146751 ],\n",
       "        [-0.27772245],\n",
       "        [-3.8455067 ],\n",
       "        [-1.8126769 ],\n",
       "        [-2.2204695 ]], dtype=float32),\n",
       " Array([[ 1.9309464 ],\n",
       "        [ 1.7242033 ],\n",
       "        [-0.42367828],\n",
       "        [-0.02427091],\n",
       "        [-2.9808059 ],\n",
       "        [ 1.0146751 ],\n",
       "        [-0.27772245],\n",
       "        [-3.8455067 ],\n",
       "        [-1.8126769 ],\n",
       "        [-2.2204695 ]], dtype=float32))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g  = grad(f)\n",
    "J1 = jacfwd(f)\n",
    "J2 = jacrev(f)\n",
    "\n",
    "g(x), J1(x), J2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XC7c41SaS3t"
   },
   "source": [
    "Let's be slightly more interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YcVxHHiHNWMD",
    "outputId": "6b8a89fd-692b-480a-8dfd-8ff1b2bf2a39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 68.2 ms, sys: 1.23 ms, total: 69.5 ms\n",
      "Wall time: 61.5 ms\n"
     ]
    }
   ],
   "source": [
    "n = int(5e3)\n",
    "m = n\n",
    "k = n\n",
    "\n",
    "key = jax.random.key(seed=0)\n",
    "A = jax.random.normal(key, (m,n))\n",
    "B = jax.random.normal(key, (k,m))\n",
    "x = jax.random.normal(key, (n,1))\n",
    "# A = jnp.array(np.random.randn(m,n)) # another (slower) way to do it\n",
    "# B = jnp.array(np.random.randn(k,m))\n",
    "# x = jnp.array(np.random.randn(n,1))\n",
    "\n",
    "def f(x):\n",
    "    return jnp.sum( nn.sigmoid(B @ nn.sigmoid(A @ x ) ) )\n",
    "\n",
    "# The first time we call the function, it is doing some overhead\n",
    "%time y = f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9yFNCfvQnpW",
    "outputId": "6a94408e-d3d8-49f4-afeb-8d661e63d79e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 840 μs, sys: 280 μs, total: 1.12 ms\n",
      "Wall time: 476 μs\n"
     ]
    }
   ],
   "source": [
    "%time y = f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-U611acvPRcv"
   },
   "outputs": [],
   "source": [
    "g  = grad(f)\n",
    "J1 = jacfwd(f)\n",
    "J2 = jacrev(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nyz6jkqfOHEW",
    "outputId": "d9feb3d4-72ba-45a8-cfc4-d8d14b1f763c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 122 ms, sys: 3.22 ms, total: 126 ms\n",
      "Wall time: 113 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y = g(x)  # reverse-mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cr2osPaOOKVJ",
    "outputId": "a3387ade-2dc6-4ad0-e8da-f76df55b895d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.18 s, sys: 121 ms, total: 1.3 s\n",
      "Wall time: 265 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y = J1(x) # forward-mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YkMddiRVPTXY",
    "outputId": "82a5b0a9-816b-41dc-c634-66bb9262efe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 877 ms, sys: 7 ms, total: 884 ms\n",
      "Wall time: 96.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y = J2(x) # reverse-mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHK4zHn3S3Tk"
   },
   "source": [
    "We see that reverse-mode (`J2` and `g`) are faster than forward mode (`J1`). Now, naively you'd expect them to be **way** faster, but I think jax is being somewhat clever about how it does the forward mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZXtWrR_qKas"
   },
   "source": [
    "## Let's repeat the same thing in PyTorch\n",
    "PyTorch is another popular autodiff framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UkXQzUcDRYnM",
    "outputId": "2139c3a1-4ce1-494f-cdb1-d61f7f49acfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version is 2.5.1\n",
      "Numpy version is 2.3.1\n",
      "Python version is 3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 11:09:21) [Clang 14.0.6 ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "from torch.nn.functional import sigmoid\n",
    "print(\"Torch version is\", torch.__version__)\n",
    "print(\"Numpy version is\", np.__version__)\n",
    "print(\"Python version is\", sys.version)\n",
    "\n",
    "torch.manual_seed(100)\n",
    "# dtype = torch.float32 # the default\n",
    "dtype = torch.float64\n",
    "\n",
    "n = int(8e3)\n",
    "m = n\n",
    "k = n\n",
    "\n",
    "A = torch.randn((m,n),dtype=dtype)\n",
    "B = torch.randn((k,m),dtype=dtype)\n",
    "x = torch.randn((n,1), dtype=dtype, requires_grad=True)\n",
    "\n",
    "def f(x):\n",
    "    return torch.sum( sigmoid(B @ sigmoid(A @ x ) ) )\n",
    "\n",
    "y = f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ll6tXP598HTY",
    "outputId": "c5ef1e5e-8d91-4550-e8b9-adaf12330856"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.1 s, sys: 15.3 ms, total: 1.11 s\n",
      "Wall time: 118 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if x.grad is not None:\n",
    "    x.grad.data.zero_()\n",
    "out = f(x)\n",
    "out.backward()\n",
    "y = x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YI3x9niGTrRF"
   },
   "source": [
    "## Speelpenning Function\n",
    "Taken from the longer [SciML AutoDiff](https://github.com/cu-applied-math/SciML-Class/blob/main/Demos/AutomaticDifferentiation.ipynb) example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 59
    },
    "id": "cukxRIAiTvwR",
    "outputId": "8987a2e1-4e2f-40a5-e0a9-e96a3915fd9d"
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x \\left(x - 1.0\\right) \\left(x - 0.888888888888889\\right) \\left(x - 0.777777777777778\\right) \\left(x - 0.666666666666667\\right) \\left(x - 0.555555555555556\\right) \\left(x - 0.444444444444444\\right) \\left(x - 0.333333333333333\\right) \\left(x - 0.222222222222222\\right) \\left(x - 0.111111111111111\\right)$"
      ],
      "text/plain": [
       "x*(x - 1.0)*(x - 0.888888888888889)*(x - 0.777777777777778)*(x - 0.666666666666667)*(x - 0.555555555555556)*(x - 0.444444444444444)*(x - 0.333333333333333)*(x - 0.222222222222222)*(x - 0.111111111111111)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy\n",
    "from sympy.abc import x\n",
    "roots = np.linspace(0,1,10)\n",
    "def g(x):\n",
    "    y = 1\n",
    "    for i in range(len(roots)):\n",
    "        y = y * (x - roots[i])\n",
    "    return y\n",
    "g(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "1tFe7aVQT8th",
    "outputId": "ac888b1d-b6a3-4705-ef36-6bf08fe665e3"
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x \\left(x - 1.0\\right) \\left(x - 0.888888888888889\\right) \\left(x - 0.777777777777778\\right) \\left(x - 0.666666666666667\\right) \\left(x - 0.555555555555556\\right) \\left(x - 0.444444444444444\\right) \\left(x - 0.333333333333333\\right) \\left(x - 0.222222222222222\\right) + x \\left(x - 1.0\\right) \\left(x - 0.888888888888889\\right) \\left(x - 0.777777777777778\\right) \\left(x - 0.666666666666667\\right) \\left(x - 0.555555555555556\\right) \\left(x - 0.444444444444444\\right) \\left(x - 0.333333333333333\\right) \\left(x - 0.111111111111111\\right) + x \\left(x - 1.0\\right) \\left(x - 0.888888888888889\\right) \\left(x - 0.777777777777778\\right) \\left(x - 0.666666666666667\\right) \\left(x - 0.555555555555556\\right) \\left(x - 0.444444444444444\\right) \\left(x - 0.222222222222222\\right) \\left(x - 0.111111111111111\\right) + x \\left(x - 1.0\\right) \\left(x - 0.888888888888889\\right) \\left(x - 0.777777777777778\\right) \\left(x - 0.666666666666667\\right) \\left(x - 0.555555555555556\\right) \\left(x - 0.333333333333333\\right) \\left(x - 0.222222222222222\\right) \\left(x - 0.111111111111111\\right) + x \\left(x - 1.0\\right) \\left(x - 0.888888888888889\\right) \\left(x - 0.777777777777778\\right) \\left(x - 0.666666666666667\\right) \\left(x - 0.444444444444444\\right) \\left(x - 0.333333333333333\\right) \\left(x - 0.222222222222222\\right) \\left(x - 0.111111111111111\\right) + x \\left(x - 1.0\\right) \\left(x - 0.888888888888889\\right) \\left(x - 0.777777777777778\\right) \\left(x - 0.555555555555556\\right) \\left(x - 0.444444444444444\\right) \\left(x - 0.333333333333333\\right) \\left(x - 0.222222222222222\\right) \\left(x - 0.111111111111111\\right) + x \\left(x - 1.0\\right) \\left(x - 0.888888888888889\\right) \\left(x - 0.666666666666667\\right) \\left(x - 0.555555555555556\\right) \\left(x - 0.444444444444444\\right) \\left(x - 0.333333333333333\\right) \\left(x - 0.222222222222222\\right) \\left(x - 0.111111111111111\\right) + x \\left(x - 1.0\\right) \\left(x - 0.777777777777778\\right) \\left(x - 0.666666666666667\\right) \\left(x - 0.555555555555556\\right) \\left(x - 0.444444444444444\\right) \\left(x - 0.333333333333333\\right) \\left(x - 0.222222222222222\\right) \\left(x - 0.111111111111111\\right) + x \\left(x - 0.888888888888889\\right) \\left(x - 0.777777777777778\\right) \\left(x - 0.666666666666667\\right) \\left(x - 0.555555555555556\\right) \\left(x - 0.444444444444444\\right) \\left(x - 0.333333333333333\\right) \\left(x - 0.222222222222222\\right) \\left(x - 0.111111111111111\\right) + \\left(x - 1.0\\right) \\left(x - 0.888888888888889\\right) \\left(x - 0.777777777777778\\right) \\left(x - 0.666666666666667\\right) \\left(x - 0.555555555555556\\right) \\left(x - 0.444444444444444\\right) \\left(x - 0.333333333333333\\right) \\left(x - 0.222222222222222\\right) \\left(x - 0.111111111111111\\right)$"
      ],
      "text/plain": [
       "x*(x - 1.0)*(x - 0.888888888888889)*(x - 0.777777777777778)*(x - 0.666666666666667)*(x - 0.555555555555556)*(x - 0.444444444444444)*(x - 0.333333333333333)*(x - 0.222222222222222) + x*(x - 1.0)*(x - 0.888888888888889)*(x - 0.777777777777778)*(x - 0.666666666666667)*(x - 0.555555555555556)*(x - 0.444444444444444)*(x - 0.333333333333333)*(x - 0.111111111111111) + x*(x - 1.0)*(x - 0.888888888888889)*(x - 0.777777777777778)*(x - 0.666666666666667)*(x - 0.555555555555556)*(x - 0.444444444444444)*(x - 0.222222222222222)*(x - 0.111111111111111) + x*(x - 1.0)*(x - 0.888888888888889)*(x - 0.777777777777778)*(x - 0.666666666666667)*(x - 0.555555555555556)*(x - 0.333333333333333)*(x - 0.222222222222222)*(x - 0.111111111111111) + x*(x - 1.0)*(x - 0.888888888888889)*(x - 0.777777777777778)*(x - 0.666666666666667)*(x - 0.444444444444444)*(x - 0.333333333333333)*(x - 0.222222222222222)*(x - 0.111111111111111) + x*(x - 1.0)*(x - 0.888888888888889)*(x - 0.777777777777778)*(x - 0.555555555555556)*(x - 0.444444444444444)*(x - 0.333333333333333)*(x - 0.222222222222222)*(x - 0.111111111111111) + x*(x - 1.0)*(x - 0.888888888888889)*(x - 0.666666666666667)*(x - 0.555555555555556)*(x - 0.444444444444444)*(x - 0.333333333333333)*(x - 0.222222222222222)*(x - 0.111111111111111) + x*(x - 1.0)*(x - 0.777777777777778)*(x - 0.666666666666667)*(x - 0.555555555555556)*(x - 0.444444444444444)*(x - 0.333333333333333)*(x - 0.222222222222222)*(x - 0.111111111111111) + x*(x - 0.888888888888889)*(x - 0.777777777777778)*(x - 0.666666666666667)*(x - 0.555555555555556)*(x - 0.444444444444444)*(x - 0.333333333333333)*(x - 0.222222222222222)*(x - 0.111111111111111) + (x - 1.0)*(x - 0.888888888888889)*(x - 0.777777777777778)*(x - 0.666666666666667)*(x - 0.555555555555556)*(x - 0.444444444444444)*(x - 0.333333333333333)*(x - 0.222222222222222)*(x - 0.111111111111111)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gprime = sympy.diff(g(x),x)\n",
    "gprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "AHNTIuBVUWOt",
    "outputId": "07c6c712-9e36-4b90-fff0-dda57258f594"
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle -0.0001040765432583041$"
      ],
      "text/plain": [
       "-0.0001040765432583041"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gprime.evalf(16,subs={x:.88889})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q80Dk__SVUno"
   },
   "source": [
    "That symbolic derivative is **correct**, but it's not an efficient implementation. We can get an efficient implementation if we play around a bit, but it's not automatic.\n",
    "\n",
    "For example, we can tell sympy to expand $g(x)$ out, and *then* differentiate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 61
    },
    "id": "MSsVDLGEVGPq",
    "outputId": "1d7c7780-d3c1-4f7d-f638-0f45b4407545"
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x^{10} - 5.0 x^{9} + 10.7407407407407 x^{8} - 12.962962962963 x^{7} + 9.64380429812528 x^{6} - 4.56104252400549 x^{5} + 1.36173159391165 x^{4} - 0.245182437937607 x^{3} + 0.0238479488367999 x^{2} - 0.000936656708416885 x$"
      ],
      "text/plain": [
       "x**10 - 5.0*x**9 + 10.7407407407407*x**8 - 12.962962962963*x**7 + 9.64380429812528*x**6 - 4.56104252400549*x**5 + 1.36173159391165*x**4 - 0.245182437937607*x**3 + 0.0238479488367999*x**2 - 0.000936656708416885*x"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sympy.expand(g(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 61
    },
    "id": "Um7mr_t3VgFr",
    "outputId": "79f492de-edf5-45be-f1fc-cce5bb14d10c"
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle -0.0001040765432661566$"
      ],
      "text/plain": [
       "-0.0001040765432661566"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gprime2 = sympy.diff( sympy.expand(g(x)), x )\n",
    "gprime2\n",
    "gprime2.evalf(16,subs={x:.88889})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzbhKuC0e_D2"
   },
   "source": [
    "## Showing that AutoDiff depends on the implementation\n",
    "\n",
    "We'll define the function $f(x)=0$ but in a slow way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "rf0F75eDVpsJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "d   = int(4e3)\n",
    "\n",
    "torch.manual_seed(100)\n",
    "A   = torch.randn( (d,d) )\n",
    "\n",
    "def f(x, N = 100):\n",
    "    \"\"\" Implements the zero function: f(x) = 0 \"\"\"\n",
    "    for k in range(N):\n",
    "        x = A @ x\n",
    "\n",
    "    return torch.sum(x - x)\n",
    "\n",
    "x   = torch.randn( (d,1), requires_grad=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iPH7qopFfKBj",
    "outputId": "4b79ddf9-fde8-4220-a5cc-68a1848de255"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.48 s, sys: 9.02 ms, total: 4.49 s\n",
      "Wall time: 489 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    y = f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYe55YIFfi9h"
   },
   "source": [
    "The gradient is the all zeros vector, but as you can see from the time it takes to execute the code, it's not being that clever..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZbJiRbGSfSMq",
    "outputId": "46110da4-604e-4aa2-c9e8-721708a2b244"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.28 s, sys: 15.3 ms, total: 5.3 s\n",
      "Wall time: 550 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y = f(x)\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JnOs0YUBffQ7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
